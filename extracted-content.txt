
--- Page 1 ---

ğŸ“”
Project Overview

--- Page 2 ---

Project Tuna Guidelines
Version 1.0
Last updated: Friday 6, 2026

--- Page 3 ---

Overview
Project Tuna asks you to evaluate two AI-generated responses to the same software
engineering task. Unlike traditional reviews that focus only on final output, this task requires you
to judge the quality, correctness, and usefulness of each response as if you were reviewing
real engineering work.
Your goal is to determine which response demonstrates stronger technical judgment and is
more suitable to move forward in a real pull request workflow.
You are not reviewing tone, politeness, or writing style.
You are evaluating substance, correctness, and execution quality.
What You Are Evaluating
For each task, you will review:
â— A task prompt and any provided context
â— Two AI-generated responses (Response A and Response B)
Your responsibility is to:
â— Identify strengths and weaknesses in each response
â— Apply standardized weakness categories when issues are present
â— Make clear, justified comparisons between the two responses
All judgments must be based on observable behavior in the responses.
Do not infer intent or speculate about what the model â€œmeant to do.â€

--- Page 4 ---

Instructions

--- Page 5 ---

Setup
Note: If you get timed out on the platform and get error 409, please open the tool in
incognito mode to keep working on the tasks or Try to use the skip button to keep
tasking.
To begin tasking on the platform, youâ€™ll receive an alias email address at your registered email.
1. Log in using your alias email at:
https://feedback.anthropic.com/surveyor/hf_pr_writer_0206_pt2?email_login=true
a. If presented with a â€œlogin with Googleâ€ option, do not choose it, it will not work.
2. Youâ€™ll receive an email verification code at your personal email.
Enter that code to complete login and start tasking.
Note: The initial assessment is just the one task on the Snorkel platform. Once that's approved
and you move to the customer platform those are the real project tasks. Please take your time
here, your work is still reviewed.
Note: if you have not received your alias, flag it to Christian Arroyo or Juan Pablo Chong
Step 1: Capability Check
At the top of the task, you will see the userâ€™s initial prompt, followed by a conversation history.
This is all just to give you context on the conversation so far. The only prompt you will
evaluate is the final user message at the bottom of the conversation field.
NOTE: the prompt you need to review will be at the bottom of the prompt.
You will have further context and files on the platform:

--- Page 6 ---

Before evaluating anything else, confirm whether you can reasonably evaluate the task.
Select â€œYes, I can evaluate this taskâ€ if:
â— The task prompt is understandable
â— Required files or context are present
â— You are familiar with the programming language or domain
Select â€œNo â€“ technical issues onlyâ€ if:
â— The task is fundamentally broken
â— The task cannot be evaluated by anyone
If you select No, you must clearly explain why the task could not be completed.
Step 2: Prompt Quality Rating (1â€“5)
Next, rate the quality of the task prompt itself.
This rating is only about the prompt, not the model responses.

--- Page 7 ---

Use the following scale:
â— 1â€“2 (Poor)
â—‹ Missing required inputs
â—‹ Unrealistic or impossible task
â—‹ Requests information the model cannot access
â— 3 (Adequate)
â—‹ Task is doable but underspecified or too simple
â—‹ Provides weak signal for evaluating an AI assistant
â— 4â€“5 (Good)
â—‹ Realistic and representative of real engineering work
â—‹ Clear requirements and sufficient context
â—‹ Challenging but solvable

--- Page 8 ---

Step 3: Weakness Categories
Read Before Evaluating Responses
Before reviewing Response A or Response B, familiarize yourself with the weakness categories
below. You will apply these categories only when they clearly apply, and you must explain
each issue precisely.
Note: The examples below are illustrative, not exhaustive. They are meant to clarify how each
category can appear, but similar issues may fall under the same category even if they are not
listed here. Evaluators should rely on their judgment, technical expertise, and the task context
when categorizing errors.
Weakness Categories
Weakness Categories with Examples
Only select categories that clearly apply. If no categories apply, omit the step.
Step 4: Evaluate Response A
Evaluate Response A on its own merits.

--- Page 9 ---

Strengths of Response A
Describe what the response does well. Focus on:
â— Correct understanding of the task
â— Meaningful progress toward a solution
â— Appropriate use of tools or assumptions
â— Alignment between claims and actual work
Weaknesses of Response A
If applicable:
â— Select the relevant weakness categories
â— Clearly explain each issue
â— Reference observable evidence in the response
If no weaknesses apply, state why.

--- Page 10 ---

Step 5: Evaluate Response B
Repeat the same evaluation process used for Response A.
Apply the same standards and level of scrutiny.

--- Page 11 ---

Step 6: Compare Responses
You will now compare Response A and Response B across several questions using a 1â€“8
scale.
Each question is independent. Do not assume the same score applies everywhere.
Comparison Questions
â— Which response is better overall?
Your holistic judgment based on correctness, usefulness, and readiness to move
forward.
â— Which code has better naming and clarity?
If no code is written, judge clarity of explanations and proposed changes.
â— Which code has better organization and modularity?
Logical structure, separation of concerns, and scalability.
â— Which code has better error handling and robustness?
Anticipation of edge cases, diagnostics, and failure modes.
â— Which code has better comments and documentation?
Useful explanations without unnecessary verbosity.
â— Which code is more ready for review or merge?
Completeness, minimal remaining gaps, and reviewability.

--- Page 12 ---

â— Which code has better logic and correctness?
Sound reasoning and alignment with the task requirements.
â— Which response is more honest about what it actually did?
Clear distinction between implemented work and proposals.
â— Which response follows the instructions better?
Full adherence to user and system constraints.
Rating Scale
â— 1â€“2: Response A is clearly better
â— 4â€“5: Responses are roughly equal
â— 7â€“8: Response B is clearly better
You must make a relative judgment, even if both responses are flawed.
Additional Comments (Optional)

--- Page 13 ---

Use this section to:
â— Call out important tradeoffs
â— Explain edge cases that influenced your decision
â— Add context that is not captured elsewhere
Do not repeat earlier critiques unless necessary.
Final Guidance
â— Judge observable behavior only
â— Penalize misleading or false claims
â— Accuracy and depth matter more than polish
â— If your evaluations are clearly justified by what the responses actually did, you are
completing the task correctly
Video walkthrough: https://www.loom.com/share/56d1157b51764db4bb80eb815960a83d
Coming Soon: Final Submission Step on Snorkel
Platform
A final submission step will be added to the Snorkel platform. This step will be used to record
and finalize your evaluation work.
Important: Completing this step will be required to receive compensation.
What to Expect
Final Submission on Snorkel Platform
After completing your evaluation tasks, return to Snorkelâ€™s platform and locate the final
submission task.
You will be asked to answer a short set of questions to confirm and complete your work.

--- Page 14 ---

Questions You May Be Asked
Expect questions similar to the following:
â— Approximately how long did it take you to complete the task?
â— Did you encounter any nuances, edge cases, or platform issues you would like to call out
(e.g., unclear instructions, UI issues, or model behavior patterns)?
All responses will be reviewed to ensure:
â— Evaluation instructions were followed correctly
â— Ratings and selections are internally consistent
â— Explanations are clear and justified
â€¼ Key takeaway: This step is confirmation-only. No additional work, files, or modifications are
required beyond what was already completed during evaluation.
This workflow is not live yet. Until it becomes available, follow only the active instructions shown
on the Snorkel platform.